{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# AI Generation of the MOT Model YAML Files \n",
    "*With IBM Granite*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## In this notebook\n",
    "This notebook contains instructions for performing YAML Model generation via custom Granite flow with Ollama.\n",
    "This notebook is heavily based by the official [IBM Granite workshop](https://ibm.github.io/granite-workshop/), for the detailed hardware setup please refer to the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the environment\n",
    "\n",
    "Ensure you are running python 3.10, 3.11, or 3.12 in a freshly-created virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granite utils provides some helpful functions for recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    transformers \\\n",
    "    langchain_community \\\n",
    "    langchain_huggingface \\\n",
    "    langchain_ollama \\\n",
    "    langchain_milvus \\\n",
    "    replicate \\\n",
    "    gitpython \\\n",
    "    requests \\\n",
    "    pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving the Granite AI model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook requires IBM Granite models to be served by an AI model runtime so that the models can be invoked or called. This notebook can use a locally accessible [Ollama](https://github.com/ollama/ollama) server to serve the models, or the [Replicate](https://replicate.com) cloud service.\n",
    "\n",
    "During the pre-work, you may have either started a local Ollama server on your computer, or setup Replicate access and obtained an [API token](https://replicate.com/account/api-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting System Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model to use for generating embedding vectors from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_path,\n",
    ")\n",
    "embeddings_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your Vector Database\n",
    "\n",
    "Specify the database to use for storing and retrieving embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus import Milvus\n",
    "import tempfile\n",
    "\n",
    "db_file = tempfile.NamedTemporaryFile(prefix=\"milvus_\", suffix=\".db\", delete=False).name\n",
    "print(f\"The vector database will be saved to {db_file}\")\n",
    "\n",
    "vector_db = Milvus(\n",
    "    embedding_function=embeddings_model,\n",
    "    connection_args={\"uri\": db_file},\n",
    "    auto_id=True,\n",
    "    index_params={\"index_type\": \"AUTOINDEX\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Select your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a Granite model to use. Here we use a Langchain client to connect to the model. If there is a locally accessible Ollama server, we use an Ollama client to access the model. Otherwise, we use a Replicate client to access the model.\n",
    "\n",
    "To use Replicate, please refer to [workshop](https://ibm.github.io/granite-workshop/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "model_path = \"ibm-granite/granite-3.3-8b-instruct\"\n",
    "model = OllamaLLM(\n",
    "    model=\"granite3.3:8b\",\n",
    "    num_ctx=65536, # 64K context window\n",
    ")\n",
    "model = model.bind(raw=True) # Client side controls prompt\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building the Vector Database\n",
    "\n",
    "Now we will input the Model name and corresponding github address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the document\n",
    "\n",
    "Here we can use any model github repo as template (plan to add model name search with web search api instead of github)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/r5/n4xqkxwd157fww2y2qptk5xr0000gn/T/ipykernel_78178/3183198176.py:27: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  return re.findall(r'https://github\\.com/[^\\s)]+', text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning https://github.com/dpfried/incoder into /var/folders/r5/n4xqkxwd157fww2y2qptk5xr0000gn/T/tmpbpqzb0a8\n",
      "\n",
      "All repo content written to 'repo_summary_input.txt'\n",
      "Feed it into Granite/Ollama to generate your YAML.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tempfile\n",
    "from git import Repo\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def clone_repo(repo_url, dest_dir=os.getcwd()):\n",
    "    print(f\"Cloning {repo_url} into {dest_dir}\")\n",
    "    Repo.clone_from(repo_url, dest_dir)\n",
    "\n",
    "def search_files_for_info(dest_dir=os.getcwd(), extensions=(\".md\", \".txt\", \".pdf\")):\n",
    "    info = []\n",
    "    for subdir, _, files in os.walk(dest_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(extensions):\n",
    "                filepath = os.path.join(subdir, file)\n",
    "                if(file.endswith('.pdf')):\n",
    "                    try:\n",
    "                        reader = PdfReader(file)\n",
    "                        content = \"\\n\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
    "                        info.append((filepath, content))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to read {filepath}: {e}\")\n",
    "                elif(file.endswith('.md') or file.endswith('.txt')):\n",
    "                    try:\n",
    "                        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                            content = f.read()\n",
    "                            info.append((filepath, content))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to read {filepath}: {e}\")\n",
    "    return info\n",
    "    \n",
    "'''experimental features for advanced search such as multi-repository\n",
    "def extract_github_links(text):\n",
    "    return re.findall(r'https://github\\.com/[^\\s)]+', text\n",
    "\n",
    "def fetch_and_store_links(links, base_dir):\n",
    "    for i, link in enumerate(set(links)):\n",
    "        name = f\"linked_repo_{i}\"\n",
    "        dest = os.path.join(base_dir, name)\n",
    "        print(f\"Cloning linked repo: {link}\")\n",
    "        try:\n",
    "            Repo.clone_from(link, dest)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to clone {link}: {e}\") '''\n",
    "\n",
    "def search_github_repo(repo_url):\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        clone_repo(repo_url, tmpdir)\n",
    "        all_info = search_files_for_info(tmpdir)\n",
    "        all_text = \"\\n\\n\".join(f\"File: {filepath.replace(tmpdir,repo_url)}\\n\\n{content}\" for filepath, content in all_info)\n",
    "\n",
    "        \n",
    "        # external_links = extract_github_links(all_text)\n",
    "        # fetch_and_store_links(external_links, tmpdir)\n",
    "\n",
    "        # Return full aggregated text content for Granite/Ollama input\n",
    "        return all_text\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "    github_repo = input(\"Enter GitHub repo URL: \").strip()\n",
    "    all_text = search_github_repo(github_repo)\n",
    "\n",
    "    with open(\"repo_summary_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_text)\n",
    "\n",
    "    print(\"\\nAll repo content written to 'repo_summary_input.txt'\")\n",
    "    print(\"Feed it into Granite/Ollama to generate your YAML.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the document into chunks\n",
    "\n",
    "Split the document into text segments that can fit into the model's context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"repo_summary_input.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=embeddings_tokenizer,\n",
    "    chunk_size=embeddings_tokenizer.max_len_single_sentence,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "doc_id = 0\n",
    "for text in texts:\n",
    "    text.metadata[\"doc_id\"] = (doc_id:=doc_id+1)\n",
    "print(f\"{len(texts)} text document chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Populate the vector database\n",
    "\n",
    "NOTE: Population of the vector database may take over a minute depending on your embedding model and service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = vector_db.add_documents(texts)\n",
    "print(f\"{len(ids)} documents added to the vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct a similarity search\n",
    "\n",
    "Search the database for similar documents by proximity of the embedded vector in vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "Given the following information about a model, fill in the YAML template with proper values.  \n",
    "If any license or path is not available, leave it blank or omit the path.  \n",
    "Ensure uniform description for each component as in the template.\n",
    "\n",
    "For components, fill license and paths if the components exist; otherwise if components do not exist omitt the sectiton,\n",
    "if license or paths is not present leave unlicensed or blank respectively.\n",
    "\n",
    "Make sure only replace '' for the model information, do not change the description.\n",
    "Only use the provided documents and do not fake it is important documents.\n",
    "\n",
    "Use the YAML format as:\n",
    "framework:\n",
    "  name: 'Model Openness Framework'\n",
    "  version: '1.0'\n",
    "  date: '2024-12-15'\n",
    "release:\n",
    "  name: ''\n",
    "  version: ''\n",
    "  date: ''\n",
    "  license:\n",
    "    distribution:\n",
    "      name: ''\n",
    "      path: ''\n",
    "    code:\n",
    "      name: ''\n",
    "      path: ''\n",
    "    data:\n",
    "      name: ''\n",
    "      path: ''\n",
    "    document:\n",
    "      name: ''\n",
    "      path: ''\n",
    "  type: ''\n",
    "  architecture: ''\n",
    "  origin: ''\n",
    "  producer: ''\n",
    "  contact: ''\n",
    "  components:\n",
    "    - name: 'Model architecture'\n",
    "      description: \"Well commented code for the model's architecture\"\n",
    "      license: unlicensed\n",
    "      component_path: ''\n",
    "      \n",
    "    - name: 'Data preprocessing code'\n",
    "      description: 'Code for data cleansing, normalization, and augmentation'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Training code'\n",
    "      description: 'Code used for training the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Inference code'\n",
    "      description: 'Code used for running the model to make predictions'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Evaluation code'\n",
    "      description: 'Code used for evaluating the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Supporting libraries and tools'\n",
    "      description: \"Libraries and tools used in the model's development\"\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Model parameters (Final)'\n",
    "      description: 'Trained model parameters, weights and biases'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Model parameters (Intermediate)'\n",
    "      description: 'Trained model parameters, weights and biases'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: Datasets\n",
    "      description: 'Training, validation and testing datasets used for the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Evaluation data'\n",
    "      description: 'Data used for evaluating the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Model metadata'\n",
    "      description: 'Any model metadata including training configuration and optimizer states'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Sample model outputs'\n",
    "      description: 'Examples of outputs generated by the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Model card'\n",
    "      description: 'Model details including performance metrics, intended use, and limitations'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Data card'\n",
    "      description: 'Documentation for datasets including source, characteristics, and preprocessing details'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Technical report'\n",
    "      description: 'Technical report detailing capabilities and usage instructions for the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Research paper'\n",
    "      description: 'Research paper detailing the development and capabilities of the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\n",
    "    - name: 'Evaluation results'\n",
    "      description: 'The results from evaluating the model'\n",
    "      license: ''\n",
    "      license_path: ''\n",
    "      component_path: ''\n",
    "\"\"\"\n",
    "docs = vector_db.similarity_search(query)\n",
    "print(f\"{len(docs)} documents returned\")\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "    print(\"=\" * 80)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automate the RAG pipeline\n",
    "\n",
    "Build a RAG chain with the model and the document retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import escape_f_string\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Create a Granite prompt for question-answering with the retrieved context\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    conversation=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"{input}\",\n",
    "    }],\n",
    "    documents=[{\n",
    "        \"doc_id\": \"0\",\n",
    "        \"text\": \"{context}\",\n",
    "    }],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "# The Granite prompt can contain JSON strings, so we must escape them\n",
    "prompt_template = PromptTemplate.from_template(template=escape_f_string(prompt, \"input\", \"context\"))\n",
    "\n",
    "# Create a Granite document prompt template to wrap each retrieved document\n",
    "document_prompt_template = PromptTemplate.from_template(template=\"\"\"\\\n",
    "<|end_of_text|>\n",
    "<|start_of_role|>document {{\"document_id\": \"{doc_id}\"}}<|end_of_role|>\n",
    "{page_content}\"\"\")\n",
    "document_separator=\"\"\n",
    "\n",
    "# Assemble the retrieval-augmented generation chain\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=prompt_template,\n",
    "    document_prompt=document_prompt_template,\n",
    "    document_separator=document_separator,\n",
    ")\n",
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=vector_db.as_retriever(),\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate a retrieval-augmented response to a question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use the RAG chain to process a question. The document chunks relevant to that question are retrieved and used as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "print(output['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
